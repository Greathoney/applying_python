# 2. 텍스트마이닝 정리

교재에서 배웠던 내용을 정리해 봅니다.



## 텍스트 마이닝의 단계

대량의 텍스트 정보를 어떻게 분석해야하는 지에 대해 알아봅시다.



### 전처리과정

텍스트를 처리하기 위해 불필요한 부분은 제거하는 작업

정교한 전처리가 이후 결과의 품질을 결정함

나만의 불용어 사전을 만든다면 더욱 높은 품질이 보장됨

#### 불용어와 불용어 처리

문서에서 빈번하게 사용되어 검색에서 무시해 버리는 문자열

##### 어간추출(Stemming)

동일한 단어의 어간을 추출하여 통일시키는 작업

nltk의 PorterStemmer 사용함

##### 토큰화(Tokenization)

텍스트 문서의 모든 문장을 개별 단어로 쪼개는 과정

공백, 모든 구두점을 제거해준다

```python
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from gensim import corpora
import re

data = pd.read_csv('wos_ai_.csv', encoding = 'euc-kr')
title = data['ABSTRACT']

words = []
words_joined = []
stopWords = set(stopwords.words('english'))
stemmer = PorterStemmer()
deleteWord = ['abcd', 'efgh']

for doc in title:
    
    ## 특정 문자 삭제
    noPunctionWords = re.sub(r"[^a-zA-Z]+", " ", str(doc)) #정규식으로 알파벳만 가져오기
    noPunctionWords = re.sub(r"[()\#$%^&,.?<>]","", str(doc)) # 또는 특수문자 제거하기
    for i in deleteWord:
	    doc = doc.replace(i, "") # 또는 내가 원하는 단어들을 제거하기 
    
    ## 모든 글자 소문자화 & 토큰화
    tokenizedWords = word_tokenize(doc.lower())
    
    ## 의미없는 단어 삭제
    stoppedWords = [w for w in tokenizedWords if w not in stopWords]
    
    ## stem처리
    stemmedWords = [stemmer.stem(w) for w in stoppedWords]
    print(stemmedWords)
    words.append(stemmedWords)
    words_joined.append(' '.join(stemmedwords))

dictionary = corpora.Dictionary(words)
corpus = [dictionary.doc2bow(word) for word in words]
```

words은 각 키워드를 담게 되고, corpus는 단어가 나타난 빈도를 담게 된다.



### 주요 키워드 분석

```python
import pandas as pd
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import numpy as np

data = pd.read_csv('wos_ai_.csv', encoding = 'euc-kr').ABSTRACT
cv = CountVectorizer(max_features = 100, stop_words = 'english')
dtm = cv.fit_transform(data)

words = cv.get_feature_names()
count_mat = dtm.sum(axis=0)
count = np.squeeze(np.asarray(count_mat))
word_count = list(zip(words, count))
word_count = sorted(word_count, key=lambda x:x[1], reverse=True)

stopwords = set(STOPWORDS)
wc = WordCloud(background_color='blue', stopwords=stopwords, width=800, height=600)
cloud = wc.generate_from_frequencies(dict(word_count))
plt.figure(figsize=(12,9))
plt.imshow(cloud)
plt.axis('off')
plt.show()
```

word_count 변수를 통해 단어의 빈도수를 쉽게 확인할 수 있다.

plt.show()의 그림을 통해 word Cloud를 확인할 수 있다.



### 키워드 간 연관분석

```python
import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt

data = pd.read_csv('wos_ai_.csv', encoding = 'euc-kr').ABSTRACT

# ~~~ 데이터 전처리 ~~~ input: data, output: words_joined

countVectorizer = CountVectorizer(max_features = 50) # 상위 50개

tdm = countVectorizer.fit_transform(words_joined)
tdm_dense = tdm.todense()
tdm_name = countVectorizer.get_feature_names()
word_corr = np.corrcoef(tdm_dense, rowvar = tdm_name)

D = pd.DataFrame(word_corr)

word_edges = []

for i in range(dtm_dense.shape[1]):
    for j in range(dtm_dense.shape[1]):
        word_edges.append((dtm_name[i], dtm_name[j], word_corr[i, j]))
        
word_edges_sorted = sorted(word_edges, key=lambda x:x[2], reverse=True)

A = pd.DataFrame(word_edges_sorted)

import networkx as nx

edgelist = np.dot(tdm_dense.T, tdm_dense)

ngraph = nx.Graph(edgelist[:, :])
ngraph_map = dict(zip(ngraph.nodes(), tdm_name))
nx.draw(ngraph, labels=ngraph_map, with_labels=True)
plt.show()
```



### 감정분석

```python
#패키지 및 데이터파일 불러오기
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

file = pd.read_csv('wos_ai_.csv', encoding='utf-8')

data = file.ABSTRACT
year = file.YEAR
doc_set = []
words = []
wordsForSentiment = []

# ~~~ 전처리 ~~~ input data, output wordForSentiment

posFile = pd.read_csv('positive-words.txt', encoding = 'utf-8')
negFile = pd.read_csv('negative-words.txt', encoding = 'utf-8')
posWords = set(posFile['POSITIVE'].tolist())
negQords = set(negFile['NEGATIVE'].tolist())
posScore = []
negScore = []
sentScore = []

for doc in wordsForSentiment:
    posS = len([w for w in doc if w in posWords])
    negS = len([w for w in doc if w in negWords])
    
```



### 키워드/문서 간 군집분석

### 토픽 찾기

### 텍스트 딥러닝

