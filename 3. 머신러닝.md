# 3. Python을 활용한 머신러닝 정리

Python을 활용한 머신러닝에 있는 교재와 강의 내용을 재구성하여 정리합니다.



## 머신러닝

머신러닝은 데이터로부터 경험을 축적하여 스스로 학습할 수 있는 모델을 연구하는 분야입니다.

어떤 현상을 예측하는 모델(시스템)을 만들 때, 고전적인 방식으로는 통계적 알고리즘을 통해 모델을 만들었습니다. (이는 Python을 활용한 데이터분석과 시각화에서 했었습니다.) 최근에는 **머신러닝 알고리즘**을 통해 모델을 만듭니다. 고전적인 방식과는 달리 머신러닝 알고리즘은 새로운 데이터를 학습하여 모델을 더욱 강화시켜 예측력을 높입니다. 데이터의 업데이트에 자동으로 적응하여 자동화가 가능하다는 점이 있습니다.

---

### 머신러닝의 종류

#### 지도학습 머신러닝

독립변수와 종속변수 모두 주어진 상태로 종속변수를 기준으로 독립변수의 차이점을 이용하여 나눌 수 있는 모델을 만들어냅니다.(Python을 활용한 데이터분석과 시각화에서의 기법을 일부 사용합니다.)

**KNN, 일반화 선형 모델, 의사결정 트리, 랜덤 포레스트, 신경망 딥러닝**이 있습니다.

#### 자율학습 머신러닝

독립변수만 주어진 상태로 독립변수의 값이 비슷한 것끼리 묶여 하나의 군집을 이룹니다. (Python을 활용한 텍스트 마이닝의 기법을 일부 사용합니다.)

**클러스터링, 차원 축소, 비정상 탐지**가 있습니다.

---

#### 머신러닝 모델이 잘 만들어질 조건

데이터의 양이 많아야 하고 질이 좋아야 합니다. (지도학습에 있어서는) 변수들이 종속변수에 관련성이 높아야 합니다. 또한 수치로 표현된 데이터는 표준화, 정규화를 적절히 해야합니다. 과대적합, 과소적합을 피하고 머신러닝 알고리즘을 다양하게 이용해 보는게 좋습니다. 

### 머신러닝 알고리즘

머신러닝 알고리즘의 절차에 대해 소개합니다.

1. 데이터 전처리과정을 거칩니다. 데이터를 **표준화**, **정규화**를 거쳐야 합니다. 이는 데이터의 단위가 다르기 때문에 거쳐야 할 부분입니다. 범주형 특성변수는 데이터 변수를 늘리고 0과 1로 나타내는 방식을 사용합니다(**one-hot-encoding**). 계산하기 힘든 고차원일 경우에는 PCA 명령어로 차원 축소할 수 있습니다.

2. 데이터 셋(Data set)을 무작위로 **학습(훈련) 데이터**, **테스트 데이터**로 나눕니다. 이때 그 비율은 7:3을 가지게 합니다. 필요하면 학습 데이터를 학습 데이터, **검증 데이터**로 나눌 수 있습니다 (7:3). 학습 데이터를 그룹으로 나누어서 교차 검증하면 좋습니다.

3. 학습 데이터와 파라미터에 영향을 받는 **머신러닝 알고리즘 모델**을 만듭니다. 그리고 검증 데이터를 이용하여 모델을 평가합니다. 정확도, 과소/과대추정이 되었는지 확인해보고 **파라미터**(머신러닝 알고리즘에 있는 상수값)을 조금씩 조정하면서 최적의 값을 찾아봅니다.

   파라미터를 연구자가 직접 정하는 Grid Search방식으로, 범위 내에서 마구잡이로 찾아보는 Random Search 방식으로 찾을 수 있습니다.

4. 파라미터 값을 조금씩 조정하다가 테스트 데이터에 가장 잘 맞는 최적의 파라미터(**Hyper Parameter**를 조정)을 찾고 그 모델로 결정합니다.

---



머신러닝의 각 절차에 대해 알아보기 전에 머신러닝 맛보기를 해 봅시다.

## K-최근접 이웃(KNN)

지도 학습중 하나로서 **데이터 분류**를 위한 기계학습 알고리즘 중 하나입니다. KNN은 각 데이터를 XYZ...축으로 두고 각 데이터 사이의 거리를 측정하여 k개의 다른 데이터의 레이블을 참조하여 분류하는 방법입니다. 보통 거리를 잴 때에는 유클리디안 거리 계산법을 사용합니다.

k값은 KNN의 **파라미터**가 되고 k가 작을수록 학습 데이터의 정확도는 높아지나, 테스트 데이터에는 부적합 할 수 있습니다(underfitting). 반대로 k가 클수록 일반화는 가능하지만 정확도가 떨어질 수 있습니다(overfitting). 보통 3~10 정도가 일반적 k수가 됩니다.

#### 특징

장점은 알고리즘이 간단하고 구현하기가 쉽습니다. 수치 기반 데이터 분류 작업에서 성능이 좋습니다.

단점은 데이터의 양이 많으면 계산 속도가 느립니다. 차원의 크기가 커지면 계산량이 많아집니다. 새로운 데이터가 들어오면 기존 데이터의 거리를 모두 계산 후 분류합니다. (인스턴트 기반 학습)

#### 명령어

X변수에 따라 투표의 유무, 어느 정당을 지지하는지 볼 수 있습니다.

```python
# 1. 데이터를 불러옵니다.
import pandas as pd
data = pd.read_csv('vote.csv')

# 2. 특성변수(X)와 레이블 변수(y)로 나눕니다.
X = data[data.columns[0:8]]
# data.loc[:, 'gender':'score_intention]
# data[['gender', 'region', ... , 'score_intention]] 도 가능합니다.
y1 = data[['vote']]
y2 = data[['parties']]

# 3. train set, test set으로 나눕니다.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y1, stratify=y1, random_state=42)
# stratify는 혹시나 레이블 변수가 특정 값 한쪽에만 너무 쏠리지 않을까 해서 골고루 뽑힐 수 있게 하는 안전장치힙니다.
# random_state는 시드입니다. 아무렇게나 써도 됩니다.

# 4. 모델을 적용합니다.
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
# knn을 사용할 Class를 가져옵니다. K수는 3으로 지정해봅니다.
knn.fit(X_train, y_train)
# <class>.fit(<train data>)는 머신러닝 알고리즘을 이용한 모델을 만드는 명령어이므로 핵심적으로 돌아가는 부분입니다. 상황에 따라서 오래 걸릴 수 있습니다.
#이제 knn은 학습이 된 모델입니다.

# 5. 모델을 평가합니다.
pred_train = knn.predict(X_train)
print(knn.score(X_train, y_train)) # output : train data 예측 확률
print(pd.crosstab(y_train.vote, pred_train)) # output : 오차행렬

pred_test = knn.predict(X_test)
print(knn.score(X_test, pred_test)) # output : test data 예측 확률
print(pd.crosstab(y_test.vote, pred_test))  # output : 오차행렬
```

----



이제 머신러닝 알고리즘 순서에 따라 봅시다.

## 1. 데이터 스케일링과 범주특성의 변환

연속형 자료에서 단위가 다를 경우 과대 혹은 과소한 파라미터가 추정되기에 동일한 기준으로 자료를 변환합니다. 키와 시력에 대한 자료를 보면 키의 값이 훨씬 평균과 표준편차가 훨씬 높게 나타나므로 자료를 변환해야합니다.

#### Min-Max Scaling

각 데이터의 값를 원래 (현재 데이터의 값 - 가장 작은 데이터의 값)을 가장 큰 값과 가장 작은 값의 차이로 나누어줍니다. (0~1 사이의 값을 가지게 됩니다.) 그 값으로 변환합니다.

#### standardization

표준화는 각 수치와 평균의 차이를 표준편차로 나눈 값입니다. 표준화로 스케일링 하면 평균이 0, 표준편차가 1이 되는 통계적인 자료 표준화의 대표적 값입니다.

#### one-hot-encoding

범주형 자료가 숫자로 되어있을 경우 여러 개의 연속형 자료로 나누는 것. 각 변수의 하위 범주에 해당되는지 아닌지를 구분하는 0과 1의 값으로 하위 범주 변수들을 생성합니다.

vote.csv 파일을 스케일링하여 봅시다.

```Python
import pandas as pd
data = pd.read_csv('vote.csv', encoding='utf-8')

#one-hot-encoding
X1 = data[['gender', 'region']]  # one-hot-encoding 할 부분
X2 = data[['edu', 'income', 'score_gov', 'score_progress', 'score_intention']]
y = data[['vote', 'parties']]  # Min-Max scaling 할 부분

X1['gender'] = X1['gender'].replace([1,2], ['male', 'female'])
X1['region'] = X1['region'].replace([1,2,3,4,5], ['Sudo', 'Chungcheung', 'Honam', 'Youngnam', 'Others'])

X1_dum = pd.get_dummies(X1) 


# Min-max scaling
from sklearn.preprocessing import MinMaxScaler
scaler1 = MinMaxScaler()
scaler1.fit(X2) 
X_scaled1 = scaler1.transform(X2)

# Standardization
from sklearn.preprocessing import StandardScaler
scaler2 =sStandradScaler()
scaler2.fit(X2)
X_scaled2 = scaler2.transform(X2)
pd.DataFrame(X_scaled2).describe()

#자료 통합 및 저장하기
import pandas as pd
X_scaled = pd.DataFrame(X_scaled1)
X_scaled.columns = ['edu', 'income', 'age', 'score_gov', 'score_progress', 'score_intention']
Fvote = pd.concat([X1_dum, X_scaled, y], axis=1)
Fvote.to_csv('Fvote.csv', sep=',', encoding='utf-8')
```



## 2. 데이터 셋 나누기

결과의 일반화 성능비교를 위해서 훈련데이터와 테스트데이터로 구분합니다. 일반적으로 훈련데이터의 교차검증과 테스트 데이터를 병행하는 것이 안정적입니다.

#### cross_val_score(랜덤없는 교차검증)

데이터를 5폴드(군집)으로 나누어서 4개의 군집은 훈련데이터로, 1개는 테스트 데이터로 하여 각 군집에 대해서 5번 반복합니다.

#### KFold(랜덤있는 교차검증)

랜덤있는 교차검증은 순서에 영향을 받는 랜덤없는 교차 검증과 달리 종속 변수(레이블)에 따라 폴드가 나누기 때문에 과소, 과대하게 분류되지 않는 장점이 있습니다.

#### shuffle-split cross validation(임의분할 교차검증)

훈련데이터와 테스트 데이터를 구성할 대 다른 교차검증에 사용되었던 데이터도 랜덤으로 선택되게 하는 방법입니다. (복원추출) 그런 과정에서 전체 데이터 중 일부는 훈련데이터 또는 테스트데이터 어디에서 선택되지 않을 수 있습니다. (미선택 데이터)

### 훈련-검증-테스트 데이터로 나누기

훈련데이터, 테스트 데이터 이외에 **검증 데이터**를 두어 일반화 경향을 더 파악하는 방법입니다. 보통 훈련데이터, 검증데이터, 테스트 데이터를 5:2:3으로 둡니다.

```python
import pandas as pd
data = pd.read_csv('Fvote.csv', encoding = 'utf-8')

X = data[data.columns[1:14]]
y = data[['vote']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)

#1 랜덤없는 교차검증
from sklearn.model_selection import cross_val_score
score1 = cross_val_score(knn, X_train, y_train, cv=5) #KNN방식(k=3)으로 머신러닝합니다. X_train과 y_train방식으로, 폴드는 순서대로 5폴드

#2 랜덤있는 교차검증
from sklearn.model_selection import KFold
kfold = KFold(n_splits=5, shuffle=True, random_state=42) #랜덤으로 섞고(이 때 시드는 42) 폴드는 5폴드
score2 = cross_val_score(knn, X_train, y_train, cv=kfold) #knn방식(k=3)으로 머신러닝합니다, 폴드는 kfold값에 따라서 합니다.

#3 임의분할 교차검증
from sklearn.model_selection import ShuffleSplit
shuffle_split = ShuffleSplit(test_size=0.5, train_size=0.5, random_state=42) #전체의 test데이터 비율, 전체의 train비율을 정합니다.
score3 = cross_val_score(knn, X_train, y_train, cv=shuffle_split) #knn방식(k=3)으로 머신러닝합니다, 폴드는 shuffle_split값에 따라서 합니다.

# ---

#4 train/test/validity 분할과 교차검증
from skelarn.model_selection import train_test_split
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state = 1)
X_train, X_vaild, y_train, y_vaild = train_test_split(X_train_val, y_train_cal, random_state=2)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
scores = cross_val_score(knn, X_train, y_train, cv=5)
knn.score(X_valid, y_valid)
knn.score(X_test, y_test) #이렇게 검증데이터, 시험데이터로 나누어서 해 볼 수가 있다.
```



## 3. 파라미터 조절 및 지정 (모델 훈련과 세부튜닝)

연구자가 일일히 파라미터 값을 지정하기에는 비효율적입니다. 그래서 컴퓨터가 다양한 파라미터를 머닝러닝 알고리즘에 넣어 가장 좋은 데이터를 집어주는 모델을 세우고자 한다.

#### 그리드 탐색

탐색하고자 하는 하이퍼파라미터를 지정하면 가능한 조합에 대한 교차검증을 사용해 평가하는 방법입니다. 분석자가 직접 하이퍼파라미터를 지정해주어야 합니다.

```python
import pandas as pd
data = pd.read_csv('Fvote.csv', encoding = 'utf-8')

X = data[data.columns[1:14]]
y = data[['vote']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

from sklearn.model_selection import GridSearchCV
param_grid = {'n_neighbors' : [1, 2, 3, 4, 5, 6, 7, 8, 9 ,10]}
from sklearn.neighbors import KNeighborsClassifier
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5) #찾는 방법은 그리드 탐색, KNN(k=1,2,3,4,5,6,7,8,9,10), 군집은 5군집 => 총 50번 반복하게 된다!
grid_search.fit(X_train, y_train) #이를 X, y 훈련 데이터에 적용하여 머신러닝 모델이 완성됩니다.

grid_search.best_params_ # output : 최적의 k값
grid_search.best_score_ # output : 최적의 k값을 사용했을때의 정확도
grid_search.score(X_test, y_test) # output : 시험데이터를 머신러닝 모델에 적용했을때의 정확도

result_gird = pd.DataFrame(grid_search.cv_results_) # 각 c값에 대해서 정확도 체크


import matplotlib.pyplot as plt
plt.plot(result_grid['param_n_neighbors'], result_grid['mean_train_score'], label='Train')
plt.plot(result_grid['param_n_neighbors'], result_grid['mean_test_score'], label = 'Test')
plt.legend()
```

#### 랜덤 탐색

분석자가 지정한 하이퍼파라미터의 조합이 아닌 지정한 범위 내에서 랜덤으로 하이퍼파라미터를 선정하여 분석하는 방법입니다. 그리드 탐색에 비해 더욱 다양한 탐색이 가능합니다.

```python
import pandas as pd
data = pd.read_csv('Fvote.csv', encoding = 'utf-8')

X = data[data.columns[1:14]]
y = data[['vote']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
#여기까지 기본 세팅

#랜덤 탐색을 위해 K값을 지정합니다
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
param_distribs = {'n_neighbors':randint(low=1, high=20)} #최소 1, 최대 20

from sklearn.neighbors import KNeighborsClassifier
random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions = param_distribs, cv = 5) #랜덤 탐색을 해라, knn방식으로, k는 param_distrub 변수에 따라 파라미터 결정, 군집은 5개 나누어서

random_search.fit(X_train, y_train)  #머신러닝 모델을 만들었음! 군집 5개에 파라미터 10개이므로 총 50번 계산함을 알 수 있다.

#각각의 output 확인
random_search.best_params_
random_search.best_score_
random_search.score(X_test, y_test)

# 교재에 없는 이 과정 꼭 해주어야 합니다!!! 그래프를 보기 위해서 정렬 작업을 해 줍니다.
result_random = random_search.cv_results_
result_random = pd.DataFrame(result_random)
result_random = result_random.sort_values('param_n_neighbors')

import matplotlib.pyplot as plt
plt.plot(result_random['param_n_neighbors'], result_random['mean_train_score'], label='Train')
plt.plot(result_random['param_n_neighbors'], result_random['mean_test_score'], label='Test')
plt.legend()
```



## 4. 모델평가

훈련된 머신러닝 알고리즘 모델을 테스트 데이터셋에 적용해 봄으로써 모델의 일반화 가능성을 평가합니다. 정확도를 통해 실제와 예측이 얼마나 일치하는가를 보아 모델의 성능을 평가합니다.

